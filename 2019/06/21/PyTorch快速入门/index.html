<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>PyTorch快速入门 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="参考：https:&#x2F;&#x2F;github.com&#x2F;chenyuntc&#x2F;pytorch-book  基本介绍Tensor t.Tensor 传入维度 - 分配空间不初始化 传入具体数据 - 舒适化   t.rand 传入维度 - 指定维度的随机初始化数据，复合[0,1]分布   Add x+y t.add(x,y) t.add(x,y,out&#x3D;result) - 指定输出目标 x.add(y)- x不变">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch快速入门">
<meta property="og:url" content="http://example.com/2019/06/21/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="参考：https:&#x2F;&#x2F;github.com&#x2F;chenyuntc&#x2F;pytorch-book  基本介绍Tensor t.Tensor 传入维度 - 分配空间不初始化 传入具体数据 - 舒适化   t.rand 传入维度 - 指定维度的随机初始化数据，复合[0,1]分布   Add x+y t.add(x,y) t.add(x,y,out&#x3D;result) - 指定输出目标 x.add(y)- x不变">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-06-21T18:15:29.000Z">
<meta property="article:modified_time" content="2019-06-27T06:09:38.000Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-PyTorch快速入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2019/06/21/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2019-06-21T18:15:29.000Z" itemprop="datePublished">2019-06-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      PyTorch快速入门
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://github.com/chenyuntc/pytorch-book">https://github.com/chenyuntc/pytorch-book</a></p>
</blockquote>
<h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><ul>
<li><code>t.Tensor</code><ul>
<li>传入维度 - 分配空间不初始化</li>
<li>传入具体数据 - 舒适化</li>
</ul>
</li>
<li><code>t.rand</code><ul>
<li>传入维度 - 指定维度的随机初始化数据，复合[0,1]分布</li>
</ul>
</li>
<li>Add<ul>
<li><code>x+y</code></li>
<li><code>t.add(x,y)</code></li>
<li><code>t.add(x,y,out=result)</code> - 指定输出目标</li>
<li><code>x.add(y)</code>- <strong>x不变</strong></li>
<li><code>x.add_(y)</code> - <strong>x改变</strong> = <code>x+=y</code></li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>函数名后面带下划线<code>_</code>会修改Tensor本身</strong></p>
</blockquote>
<ul>
<li>与Numpy转换：<ul>
<li>Tensor -&gt; numpy: <code>a.numpy()</code></li>
<li>Numpy -&gt; tensor: <code>t.from_numpy(a)</code></li>
</ul>
</li>
</ul>
<blockquote>
<p>一个对象的tensor和numpy共享内存，一个改变，另一个随之而变</p>
</blockquote>
<ul>
<li><p>获得元素值：</p>
<ul>
<li>下标访问tensor - <code>tensor[index...]</code>获得<strong>0-dim tensor</strong></li>
<li>获得值 - <code>scalar.item()</code></li>
</ul>
</li>
<li><p>scalar和tensor的区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tensor = t.tensor([<span class="number">2</span>])</span><br><span class="line">scalar = tensor[<span class="number">0</span>]</span><br><span class="line">scalar0 = t.tensor(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># tensor 1-dim</span></span><br><span class="line">tensor.size() <span class="comment"># torch.Size([1])</span></span><br><span class="line"><span class="comment"># scalar 0-dim</span></span><br><span class="line">scalar.size() <span class="comment"># torch.Size([])</span></span><br><span class="line"><span class="comment"># scalar0 0-dim</span></span><br><span class="line">scalar0.size() <span class="comment"># torch.Size([])</span></span><br></pre></td></tr></table></figure></li>
<li><p>数据拷贝和共享内存</p>
<ul>
<li><code>t.tensor()</code> - 会进行数据拷贝</li>
<li><code>t.from_numpy()</code> / <code>tensor.detach()</code>新建的tensor与原tensor共享内存</li>
</ul>
</li>
<li><p>GPU加速</p>
<ul>
<li><code>t.cuda</code></li>
</ul>
</li>
</ul>
<h3 id="Autograd-自动微分"><a href="#Autograd-自动微分" class="headerlink" title="Autograd: 自动微分"></a>Autograd: 自动微分</h3><ul>
<li><p>使用autograd功能</p>
<ul>
<li><p>```python<br>x = t.ones(2, 2, requires_grad=True)<br>result = doSomething(x)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 反向传播</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    result.backward()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>反向传播会累加之前的梯度，反向传播前把梯度清零</p>
</blockquote>
</li>
<li><p>梯度清零</p>
<ul>
<li>```python<br>x.grad.data.zero_()<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 神经网络</span><br><span class="line"></span><br><span class="line">定义网络</span><br><span class="line"></span><br><span class="line">- 继承`nn.Module`，实现`forward`，把网络中具有**可学习参数**的层放在`__init__`中。不具有可学习参数使用`forward`的`nn.functional`代替</span><br><span class="line"></span><br><span class="line">- `__init__`函数先执行父类构造函数`super(CLASSNAME, self).__init__()`</span><br><span class="line"></span><br><span class="line">##### 卷积层</span><br><span class="line"></span><br><span class="line">- `nn.Conv2d(inputC, outputC, k)`</span><br><span class="line">  - 输入图片通道数</span><br><span class="line">  - 输出通道数</span><br><span class="line">  - 卷积核</span><br><span class="line"></span><br><span class="line">##### 全连接层</span><br><span class="line"></span><br><span class="line">- `nn.Linear(input, output)`</span><br><span class="line"></span><br><span class="line">  - 输入样本数</span><br><span class="line">  - 输出样本数</span><br><span class="line"></span><br><span class="line">  &gt; y = Wx + b</span><br><span class="line"></span><br><span class="line">##### forward</span><br><span class="line"></span><br><span class="line">- `F.relu(input)` - 激活</span><br><span class="line">  - 上一层的结果</span><br><span class="line">- `F.max_pool2d(input, kernel size)`</span><br><span class="line">  - kernel size</span><br><span class="line"></span><br><span class="line">- `x.view(ONEDIMENSION,-1`</span><br><span class="line">  - -1 自适应维度</span><br><span class="line"></span><br><span class="line">#### 训练网络</span><br><span class="line"></span><br><span class="line">- 定义了forward函数，backward就会自动被实现</span><br><span class="line">- `net.parameters()`返回网络的**可学习参数**</span><br><span class="line">- `·net.named_parameters()`返回可学习的参数及名称</span><br><span class="line"></span><br><span class="line">&gt; torch.nn不支持一次只输入一个样本：</span><br><span class="line">&gt;</span><br><span class="line">&gt; 1. 用 `input.unsqueeze(0)`将batch_size设为１</span><br><span class="line">&gt; 2. Conv2d输入时将nSample设为1</span><br><span class="line"></span><br><span class="line">#### 损失函数</span><br><span class="line"></span><br><span class="line">- `nn.MSELoss(output, target)`计算均方误差</span><br><span class="line">- `nn.CrossEntropyLoss(output target)`计算交叉熵损失</span><br><span class="line"></span><br><span class="line">#### 优化器</span><br><span class="line"></span><br><span class="line">- 反向传播计算参数的梯度，使用优化方法**更新网络的权重和参数**</span><br><span class="line"></span><br><span class="line">- e.g. SGD：</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    weight = weight - learning_rate * gradient</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>```python<br>learning_rate = 0.01<br>for f in net.parameters():</p>
<pre><code>f.data.sub_(f.grad.data * learning_rate)# inplace 减法
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 构造优化器</span><br><span class="line"></span><br><span class="line">&gt; optim &lt;= torch.optim</span><br><span class="line"></span><br><span class="line">- ```</span><br><span class="line">  optimizer = optim.SGD(net.parameters(), lr = 0.01)</span><br></pre></td></tr></table></figure></li>
<li><p>要调整的参数为网络中的可学习参数</p>
</li>
<li><p>lr为学习率</p>
</li>
</ul>
</li>
<li><p><strong>训练步骤</strong>：</p>
<ol>
<li><p>清零优化器梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure></li>
<li><p>过网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>计算损失</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(output, target)</span><br></pre></td></tr></table></figure></li>
<li><p>反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></table></figure></li>
<li><p>更新参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
</ul>
<h2 id="Tensor-和-Autograd"><a href="#Tensor-和-Autograd" class="headerlink" title="Tensor 和 Autograd"></a>Tensor 和 Autograd</h2><h3 id="Tensor-1"><a href="#Tensor-1" class="headerlink" title="Tensor"></a>Tensor</h3><h4 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h4><h5 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h5><ul>
<li><table>
<thead>
<tr>
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody><tr>
<td>arange(s,e,step)</td>
<td>从s到e，步长为step</td>
</tr>
<tr>
<td>linspace(s,e,steps)</td>
<td>从s到e，均匀切分成steps份</td>
</tr>
<tr>
<td>rand/randn(*sizes)</td>
<td>均匀/标准分布</td>
</tr>
<tr>
<td>normal(mean,std)/uniform(from,to)</td>
<td>正态分布/均匀分布</td>
</tr>
<tr>
<td>randperm(m)</td>
<td>随机排列</td>
</tr>
</tbody></table>
</li>
<li><p>创建的时候可以指定数据类型(e.g. <code>dtype = t.int</code>)和存放device(e.g. <code>device = t.device(&#39;cpu&#39;)</code></p>
</li>
<li><p>tensor转化为list - <code>tensor.tolist()</code></p>
<blockquote>
<p>tensor的数据类型为<code>tensor([[1,2],[3,4]])</code>; list数据类型为<code>[[1,2],[3,4]]</code></p>
</blockquote>
</li>
<li><p>获得tensor的维度 - <code>tensor.size()</code>或<code>tensor.shape</code>返回torch.Size对象 - <code>torch.Size([第一维, 第二维, ..])</code></p>
</li>
<li><p>获得tensor元素的总数 - <code>tensor.numel()</code></p>
</li>
<li><p>传参为数据和维度的区别：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t.Tensor(<span class="number">2</span>,<span class="number">3</span>) <span class="comment"># tensor([[x, x, x],[x, x, x]])</span></span><br><span class="line">t.Tensor((<span class="number">2</span>,<span class="number">3</span>)) <span class="comment"># tensor([2,3])</span></span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>使用维度作为参数构造tensor的时候不会马上分配空间，使用到tensor才分配；其他的构造函数都是马上分配</p>
</blockquote>
<ul>
<li>使用<code>eye</code>构造对角矩阵，不要求行列数一致</li>
</ul>
<h5 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h5><ul>
<li><p>调整tensor形状，调整前后<strong>元素总数一致</strong>。view返回的新tensor与原tensor共享内存。</p>
</li>
<li><p><code>view</code>的参数为修改后的每个维度，如果有一个为<code>-1</code>则该维度根据元素总数不变的原则自动计算</p>
</li>
<li><p><code>unsqueeze(index)</code> - 第index维的维度+1</p>
<ul>
<li>即之前维度为[x, y, z, …]<ul>
<li>index = 0 -&gt; 维度变为 [1, x, y, z, …]</li>
<li>index = 1 -&gt; 维度变为 [x, 1, y, z, …]</li>
<li>index = 2 -&gt; 维度变为 [x, y, 1, z, …]</li>
</ul>
</li>
<li>参数为负数的时候表示倒数</li>
</ul>
</li>
<li><p><code>squeeze(index)</code> - 压缩第index维的1，如果不输入index，则把所有维度为1的压缩</p>
</li>
<li><p><code>resize</code>可以修改tensor的大小，如果新大小超过了原大小，自动分配新空间，大小小于原大小，之前数据仍然保留</p>
</li>
</ul>
<h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><ul>
<li><p>索引出来的结果与原tensor共享内存</p>
</li>
<li><p><code>None</code> - 新增轴</p>
<ul>
<li><p>```pythob<br>a[:,None,:,None,None].shape<br>torch.Size([3, 1, 4, 1, 1])</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 筛选tensor元素 - `tensor[CONDITION]`</span><br><span class="line"></span><br><span class="line">  &gt; 返回结果与原tensor不共享内存空间</span><br><span class="line"></span><br><span class="line">- 常用选择函数</span><br><span class="line"></span><br><span class="line">  |                            函数 |                                                  功能 |</span><br><span class="line">  | ------------------------------: | ----------------------------------------------------: |</span><br><span class="line">  | index_select(input, dim, index) |           在指定维度dim上选取，比如选取某些行、某些列 |</span><br><span class="line">  |      masked_select(input, mask) |              例子如上，a[a&gt;0]，使用ByteTensor进行选取 |</span><br><span class="line">  |                 non_zero(input) |                                         非0元素的下标 |</span><br><span class="line">  |       gather(input, dim, index) | 根据index，在dim维度上选取数据，输出的size与index一样 |</span><br><span class="line"></span><br><span class="line">&gt; 对tensor的任何索引操作仍是一个**tensor**，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法**只对包含一个元素的tensor适用**</span><br><span class="line"></span><br><span class="line">- 索引访问：</span><br><span class="line">  - 维度为3：[[a,b], [c,d], [e,f]] =&gt; [[a,c,e], [b,d,f]]</span><br><span class="line">  - [[a,b,c], [d], [e]] =&gt; [[a,d,e], [b,d,e], [c,d,e]]</span><br><span class="line"></span><br><span class="line">##### 逐元素操作</span><br><span class="line"></span><br><span class="line">- `clamp` - 控制元素范围，用于比较大小</span><br><span class="line"></span><br><span class="line">  ![1561187765542](C:\Users\Sherry\AppData\Roaming\Typora\typora-user-images\1561187765542.png)</span><br><span class="line"></span><br><span class="line">##### 归并操作</span><br><span class="line"></span><br><span class="line">- 使得输出形状小于输入形状，e.g. 均值、和...</span><br><span class="line">- 指定对第几维度的元素操作，`keepdim = True`保存被操作的维度为1，否则不保留这一维度</span><br><span class="line"></span><br><span class="line">##### 线性代数</span><br><span class="line"></span><br><span class="line">| 函数                             | 功能                              |</span><br><span class="line">| -------------------------------- | --------------------------------- |</span><br><span class="line">| trace                            | 对角线元素之和(矩阵的迹)          |</span><br><span class="line">| diag                             | 对角线元素                        |</span><br><span class="line">| triu/tril                        | 矩阵的上三角/下三角，可指定偏移量 |</span><br><span class="line">| mm/bmm                           | 矩阵乘法，batch的矩阵乘法         |</span><br><span class="line">| addmm/addbmm/addmv/addr/badbmm.. | 矩阵运算                          |</span><br><span class="line">| t                                | 转置                              |</span><br><span class="line">| dot/cross                        | 内积/外积                         |</span><br><span class="line">| inverse                          | 求逆矩阵                          |</span><br><span class="line">| svd                              | 奇异值分解                        |</span><br><span class="line"></span><br><span class="line">#### Tensor和Numpy</span><br><span class="line"></span><br><span class="line">&gt; 当numpy的数据类型和Tensor的类型不一样时，数据会被复制，不共享内存</span><br><span class="line"></span><br><span class="line">- 调用`t.tensor`进行构建的时候都会进行数据拷贝</span><br><span class="line"></span><br><span class="line">##### 广播法则</span><br><span class="line"></span><br><span class="line">###### N</span><br><span class="line"></span><br><span class="line">- 所有输入数组像其中shape最长的看齐，shape不足通过前面加1</span><br><span class="line">- 两个数组要么在某一个维度长度一致，要么其中一个为1，否则不能计算</span><br><span class="line">- 输入某个长度为1，计算时沿此维度扩充成和另一个输入一样的维度</span><br><span class="line"></span><br><span class="line">###### Pytorch中的使用</span><br><span class="line"></span><br><span class="line">- `unsqueeze`或者`view`或者`tensor[NONE]`，实现补齐1</span><br><span class="line"></span><br><span class="line">- `expand`或者`expand_as`重复数组，不会占用空间</span><br><span class="line"></span><br><span class="line">  &gt; repeat和expand相似，repeat占用额外空间，expand不占用</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">#### 内部结构</span><br><span class="line"></span><br><span class="line">- Tensor分为头信息区tensor和存储区storage</span><br><span class="line">  - tensor保存size, stride, type</span><br><span class="line">  - 共享内存的情况下，即共享storage</span><br><span class="line">- 下标访问只是对于原storage地址增加了偏移量进行映射的</span><br><span class="line"></span><br><span class="line">&gt; 大部分操作不修改tensor，只修改tensor的头</span><br><span class="line">&gt;</span><br><span class="line">&gt; contiguous方法将离散的tensor变成连续数据</span><br><span class="line">&gt;</span><br><span class="line">&gt; 高级索引一般不共享storage，普通索引共享storage</span><br><span class="line"></span><br><span class="line">#### 其他问题</span><br><span class="line"></span><br><span class="line">##### GP</span><br><span class="line"></span><br><span class="line">- 推荐使用`tensor.to(device)`使得程序同时兼容CPU和GPU，避免频繁在内存和显存中传输数据</span><br><span class="line"></span><br><span class="line">##### 持久化</span><br><span class="line"></span><br><span class="line">- `t.save`, </span><br><span class="line"></span><br><span class="line">##### 向量化</span><br><span class="line"></span><br><span class="line">- python的for循环十分低效</span><br><span class="line">- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目</span><br><span class="line"></span><br><span class="line">### AutoGrad</span><br><span class="line"></span><br><span class="line">- 自动求导引擎</span><br><span class="line"></span><br><span class="line">#### 计算图</span><br><span class="line"></span><br><span class="line">- 有向无环图</span><br><span class="line"></span><br><span class="line">- 有向无环图的叶子节点由用户自己创建，不依赖其他变量，根节点为计算图的最终目标</span><br><span class="line"></span><br><span class="line">- 链式的中间导数在前向传播过程中保存为buffer，在计算完梯度后会自动清空</span><br><span class="line"></span><br><span class="line">  &gt; 需要多次反向传播则指定`retain_graph`保存buffer</span><br><span class="line"></span><br><span class="line">- 变量的`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都是True。</span><br><span class="line"></span><br><span class="line">- 修改tensor数值，不希望被autograd记录，则修改tensor.data或者tensor.detach()</span><br><span class="line">- 反向传播非叶子结点的导数会在计算完后被清空，查看梯度可以使用autograd.grad函数或者hood</span><br><span class="line">  </span><br><span class="line">  - hook需要register和remove</span><br><span class="line"></span><br><span class="line">##### 总结</span><br><span class="line"></span><br><span class="line">- `autograd`根究用户对variable的操作构建计算图。对变量的操作抽象为Function</span><br><span class="line">- 某个变量不是Function的输出，且由用户创建则为叶子节点（用户输入），其`grad_fn`为None. 叶子节点中需要求导的Variable，具有`AccumulateGrad`标识，因为梯度累加</span><br><span class="line">- Variable默认不求导，如果一个节点的`require_grad`设置为True，所有依赖它的节点`require_grad`为True</span><br><span class="line">- Variable的`volatile`默认为True，volatile为True的节点不会求导，volatile优先级比`require_grad`高</span><br><span class="line">- 多次反向传播梯度累加，若需要保存反向传播中间变量的值，需要设置`retain_graph = True`</span><br><span class="line"></span><br><span class="line">- 非叶子节点梯度计算完之后被清空，使用`autogtad.grad`或`hook`获取</span><br><span class="line"></span><br><span class="line">#### 扩展autograd</span><br><span class="line"></span><br><span class="line">- 自己定义函数继承`Function`，并且实现`forward`, `backward`静态方法（def之前写`@staticmethod`）</span><br><span class="line"></span><br><span class="line">- 主函数中调用时：</span><br><span class="line"></span><br><span class="line">  - 前向传播：</span><br><span class="line"></span><br><span class="line">    - ```</span><br><span class="line">      z = MultiplyAdd.apply(w, x, b)</span><br></pre></td></tr></table></figure></li>
<li><p>反向传播:</p>
<ul>
<li>```<br>z.backward()<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 神经网络工具箱nn</span><br><span class="line"></span><br><span class="line">- Module - 神经网络中的某层/包含很多层的神经网络</span><br><span class="line"></span><br><span class="line">##### 全连接层</span><br><span class="line"></span><br><span class="line">- 继承`nn.Module`</span><br><span class="line"></span><br><span class="line">- 构造函数`__init__`自己定义可学习参数</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  # infeatures和outfeatures为输入输出特征的维度</span><br><span class="line">  def __init__(self, infeatures, outfeatures):</span><br><span class="line">  	super...</span><br><span class="line">  	# 自己定义可学习参数如下：</span><br><span class="line">  	self.para1 = nn.Parameter..</span><br><span class="line">  	self.para2 = ...</span><br><span class="line">  	...</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<blockquote>
<p><code>nn.Parameter</code> - tensor，默认<code>requires_grad = True</code></p>
</blockquote>
</li>
<li><p>不需要写反向传播函数</p>
</li>
<li><p>调用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">layer = Linear(INFEATURES, OUTFEATURES)</span><br><span class="line">y = layer(x)</span><br></pre></td></tr></table></figure></li>
<li><p>返回可学习参数 - <code>named_parameters()</code></p>
</li>
</ul>
<h5 id="子模块sub-module"><a href="#子模块sub-module" class="headerlink" title="子模块sub module"></a>子模块sub module</h5><ul>
<li><code>named_parameters()</code>返回可学习参数为：模块名.参数名</li>
</ul>
<h3 id="常用神经网络层"><a href="#常用神经网络层" class="headerlink" title="常用神经网络层"></a>常用神经网络层</h3><h4 id="图像相关层"><a href="#图像相关层" class="headerlink" title="图像相关层"></a>图像相关层</h4><h5 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h5><ul>
<li><code>nn.Conv2d</code></li>
</ul>
<h5 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h5><ul>
<li>没有可学习参数，权重固定</li>
</ul>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><ul>
<li><p><code>Linear</code>全连接</p>
</li>
<li><p><code>BatchNorm</code> - 批量规范化，风格迁移<code>instanceNorm</code></p>
<ul>
<li>初始化通道数</li>
<li>权重初始化为单位阵</li>
<li>偏差初始化为0</li>
</ul>
</li>
<li><p><code>Dropout</code> - 防止过拟合</p>
<ul>
<li>输出每个元素的舍弃概率</li>
</ul>
</li>
</ul>
<h4 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h4><h5 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h5><ul>
<li>$ReLU(x) = max(0, x)$<ul>
<li>其inplace参数为True会将输出覆盖到输入，节省内存，一般不使用Inplace</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>前馈传播网络 feedfoward neural network</strong>： 将每一层的输出直接作为下一层的输入。简化forward -&gt; ModuleList &amp; Sequential</p>
</blockquote>
<h6 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h6><ul>
<li>包含几个sub module - 前向传播一层一层传递</li>
</ul>
<ol>
<li>声明，添加module</li>
</ol>
<ul>
<li>```python<br>net1 = nn.Sequential()<br>net1.add_module(‘conv’, nn.Conv2d(3,3,3))<br>net1.add_module(‘batchnorm’, nn.BatchNorm2d(3))<br>net1.add_module(‘activation_layer’, nn.ReLU())<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2. 将模块作为参数传递</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  net2 = nn.Sequential(</span><br><span class="line">          nn.Conv2d(3, 3, 3),</span><br><span class="line">          nn.BatchNorm2d(3),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">          )</span><br></pre></td></tr></table></figure></li>
</ul>
<ol start="3">
<li>将模块作为有序字典传入</li>
</ol>
<ul>
<li>```python<br>from collections import OrderedDict<br>net3= nn.Sequential(OrderedDict([<pre><code>      (&#39;conv1&#39;, nn.Conv2d(3, 3, 3)),
      (&#39;bn1&#39;, nn.BatchNorm2d(3)),
      (&#39;relu1&#39;, nn.ReLU())
    ]))
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- 其中方法1和方法3可以按照模块名称访问子模块，而方法2按照下标获取子模块（从0）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">- 模块数组`nn.ModuleList`，其中的元素可以被主（父）module识别，即其参数可以自动加入到主module的参数中</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  modellist = nn.ModuleList([nn.Linear(3,4), nn.ReLU(), nn.Linear(4,2)])</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><ul>
<li><p>```<br>nn.LOSS_FUNCTION_NAME()</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 优化器</span><br><span class="line"></span><br><span class="line">- 优化方法来自包`torch.optim`，所有的优化方法继承基类`optim.Optimizer`</span><br><span class="line"></span><br><span class="line">- 优化器可以对于不同的子网络（如在主网络/主Module中定义了多个子模块或者Sequential）</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    optimizer =optim.SGD([</span><br><span class="line">                    &#123;&#x27;params&#x27;: net.features.parameters()&#125;, # 学习率为1e-5</span><br><span class="line">                    &#123;&#x27;params&#x27;: net.classifier.parameters(), &#x27;lr&#x27;: 1e-2&#125;</span><br><span class="line">                ], lr=1e-5)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>如果某个参数显示执行学习率，则使用最外层的默认学习率</p>
</blockquote>
</li>
<li><p>指定自网络中的不同层有不同的学习率的例子：</p>
<ul>
<li>```python<h1 id="只为两个全连接层设置较大的学习率，其余层的学习率较小"><a href="#只为两个全连接层设置较大的学习率，其余层的学习率较小" class="headerlink" title="只为两个全连接层设置较大的学习率，其余层的学习率较小"></a>只为两个全连接层设置较大的学习率，其余层的学习率较小</h1>special_layers = nn.ModuleList([net.classifier[0], net.classifier[3]])<br>special_layers_params = list(map(id, special_layers.parameters()))<br>base_params = filter(lambda p: id(p) not in special_layers_params,<pre><code>                 net.parameters())
</code></pre>
optimizer = t.optim.SGD([<pre><code>        &#123;&#39;params&#39;: base_params&#125;,
        &#123;&#39;params&#39;: special_layers.parameters(), &#39;lr&#39;: 0.01&#125;
    ], lr=0.001 )
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### 调整学习率</span><br><span class="line"></span><br><span class="line">###### 新建优化器</span><br><span class="line"></span><br><span class="line">- 优点：简单</span><br><span class="line">- 缺点：如果使用动量的优化器，会丢失动量等状态信息</span><br><span class="line"></span><br><span class="line">###### 修改学习率</span><br><span class="line"></span><br><span class="line">- 手动decay保存动量</span><br><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  for param_group in optimizer.param_groups:</span><br><span class="line">      param_group[&#x27;lr&#x27;] *= 0.1 # 学习率为之前的0.1倍</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="nn-functional"><a href="#nn-functional" class="headerlink" title="nn.functional"></a>nn.functional</h3><ul>
<li>大部分layer在<code>functional</code>有与之对应的函数</li>
</ul>
<h6 id="nn-functional和nn-Module的区别"><a href="#nn-functional和nn-Module的区别" class="headerlink" title="nn.functional和nn.Module的区别"></a>nn.functional和nn.Module的区别</h6><ul>
<li>module: 自动提取可学习参数</li>
<li>function: 类似纯函数，不需要可学习参数时可使用. e.g. 激活函数，池化层</li>
</ul>
<hr>
<ul>
<li>不具备可学习参数的层不放在构造函数<code>__init__</code>中。</li>
</ul>
<h3 id="初始化策略"><a href="#初始化策略" class="headerlink" title="初始化策略"></a>初始化策略</h3><ul>
<li>随机初始化可能导致后期训练中梯度爆炸或梯度消失</li>
<li><code>nn.init</code>模块</li>
</ul>
<h3 id="nn-Module深入分析"><a href="#nn-Module深入分析" class="headerlink" title="nn.Module深入分析"></a>nn.Module深入分析</h3><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><table>
<thead>
<tr>
<th>变量名</th>
<th>类型</th>
<th>说明</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td><code>_parameters</code></td>
<td>字典</td>
<td>用户设置</td>
<td>子模块的参数不会存放于此</td>
</tr>
<tr>
<td><code>_modules</code></td>
<td>字典</td>
<td>子模块</td>
<td></td>
</tr>
<tr>
<td><code>_buffers</code></td>
<td>字段</td>
<td>缓存</td>
<td></td>
</tr>
<tr>
<td><code>_backward_hooks</code>与<code>_forward_hooks</code></td>
<td></td>
<td>钩子，提取中间变量</td>
<td></td>
</tr>
<tr>
<td><code>training</code></td>
<td></td>
<td>区分训练阶段于测试阶段，据此确定前向传播策略</td>
<td></td>
</tr>
</tbody></table>
<h5 id="parameters-modules-buffers"><a href="#parameters-modules-buffers" class="headerlink" title="_parameters, _modules, _buffers"></a><code>_parameters</code>, <code>_modules</code>, <code>_buffers</code></h5><ul>
<li><p>使用<code>named_paramaters()</code>的时候将parameters和modules的parameter全部返回</p>
</li>
<li><p>module层层嵌套，常用方法：</p>
<ul>
<li><code>named_children</code>- 查看<strong>直接</strong>子module</li>
<li><code>named_modules</code> - 查看所有子module</li>
</ul>
</li>
</ul>
<hr>
<h5 id="training"><a href="#training" class="headerlink" title="training"></a><code>training</code></h5><ul>
<li>一些layer在训练和测试阶段差距较大，为了使得每个<code>training</code>都要被设置好 =&gt; 定义了模型的的train和eval模式<ul>
<li><code>model.train()</code> - 将当前Module及其子module所有<code>training</code>属性设置为True</li>
<li><code>model.eval()</code> - 将training属性都设为False</li>
</ul>
</li>
</ul>
<h5 id="backward-hooks与-forward-hooks"><a href="#backward-hooks与-forward-hooks" class="headerlink" title="_backward_hooks与_forward_hooks"></a><code>_backward_hooks</code>与<code>_forward_hooks</code></h5><ul>
<li>在module前向传播或反向传播时注册，传播执行结束执行钩子，使用完及时删除</li>
<li>钩子用于获取某些中间结果</li>
</ul>
<h6 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span>(<span class="params">module, <span class="built_in">input</span>, output</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;把这层的输出拷贝到features中&#x27;&#x27;&#x27;</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line"></span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(<span class="built_in">input</span>)</span><br><span class="line"><span class="comment"># 用完hook后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure>

<hr>
<p>Python的原始实现：</p>
<ul>
<li>result = obj.name会调用buildin函数<code>getattr(obj, &#39;name&#39;)</code>，如果该属性找不到，会调用<code>obj.__getattr__(&#39;name&#39;)</code></li>
<li>obj.name = value会调用buildin函数<code>setattr(obj, &#39;name&#39;, value)</code>，如果obj对象实现了<code>__setattr__</code>方法，<code>setattr</code>会直接调用<code>obj.__setattr__(&#39;name&#39;, value&#39;)</code></li>
</ul>
<p>在<code>nn.Module</code>中的实现：</p>
<ul>
<li>在<code>nn.Module</code>中实现了<code>__setattr__</code>函数，执行<code>module.name = value</code>时，在<code>__setattr__</code>中判断value是否为<code>Parameter</code>或<code>nn.Module</code>对象。如果是则加入<code>_parameters</code>和<code>_modules</code>字典。如果是其他的对象，则保存在<code>__dict__</code></li>
</ul>
<blockquote>
<p><code>_modules</code>和<code>_parameters</code>的item未保存在<code>__dict__</code>中，默认<code>getattr</code>无法获取，<code>nn.Module</code>实现<code>__getattr__</code>：如果<code>getattr</code>无法处理，则调用自定义的<code>__getattr__</code>从<code>_modules</code>, <code>_parameters</code>和<code>_buffers</code>三个字典中获取</p>
</blockquote>
<h5 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h5><ul>
<li><p>使用Module的<code>state_dict()</code>函数，返回当前Module所有状态数据。下次使用，<code>module.load_state_dict()</code></p>
</li>
<li><p>优化器的实现：</p>
<ul>
<li><pre><code class="python"># 保存模型
t.save(net.state_dict(), &#39;net.pth&#39;)

# 加载已保存的模型
net2 = Net()
net2.load_state_dict(t.load(&#39;net.pth&#39;))
</code></pre>
</li>
</ul>
</li>
<li><p>将Module放在GPU上运行</p>
<ul>
<li><code>model = model.cuda()</code> 将所有参数转存到GPU</li>
<li><code>input.cuda()</code> 将输入放入GPU</li>
</ul>
</li>
</ul>
<h5 id="在多个GPU计算"><a href="#在多个GPU计算" class="headerlink" title="在多个GPU计算"></a>在多个GPU计算</h5><ul>
<li><h2 id="nn-parallel-data-parallel-module-inputs-device-ids-None-output-device-None-dim-0-module-kwargs-None"><a href="#nn-parallel-data-parallel-module-inputs-device-ids-None-output-device-None-dim-0-module-kwargs-None" class="headerlink" title="nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)"></a><code>nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)</code></h2></li>
<li><code>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)</code><ul>
<li>将一个输入batch分成多分，送到对应GPU计算，各个GPU得到的梯度累加</li>
</ul>
</li>
</ul>
<blockquote>
<p>通过<code>device_ids</code>参数指定在哪些GPU上优化，<code>output_devices</code>指定输出到哪个GPU</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2019/06/21/PyTorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" data-id="ckt2e2y1j000epnutd0yh1n00" data-title="PyTorch快速入门" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/23/Pytorch%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%80%BB%E7%BB%93/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Pytorch常用工具总结
        
      </div>
    </a>
  
  
    <a href="/2019/06/20/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">python学习笔记</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-GAME-DESIGN/" rel="tag">3D GAME DESIGN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Graphics/" rel="tag">Computer Graphics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/" rel="tag">Data Mining</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github/" rel="tag">Github</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode/" rel="tag">LeetCode</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markdown/" rel="tag">Markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MockingBot/" rel="tag">MockingBot</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Software-Analysis-and-Design/" rel="tag">Software Analysis and Design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Software-System-Analysis-and-Design/" rel="tag">Software System Analysis and Design</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Software-Testing/" rel="tag">Software Testing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/3D-GAME-DESIGN/" style="font-size: 20px;">3D GAME DESIGN</a> <a href="/tags/Computer-Graphics/" style="font-size: 14px;">Computer Graphics</a> <a href="/tags/Data-Mining/" style="font-size: 12px;">Data Mining</a> <a href="/tags/Github/" style="font-size: 10px;">Github</a> <a href="/tags/LeetCode/" style="font-size: 10px;">LeetCode</a> <a href="/tags/Markdown/" style="font-size: 10px;">Markdown</a> <a href="/tags/MockingBot/" style="font-size: 10px;">MockingBot</a> <a href="/tags/Software-Analysis-and-Design/" style="font-size: 10px;">Software Analysis and Design</a> <a href="/tags/Software-System-Analysis-and-Design/" style="font-size: 18px;">Software System Analysis and Design</a> <a href="/tags/Software-Testing/" style="font-size: 10px;">Software Testing</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/pytorch/" style="font-size: 16px;">pytorch</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/02/%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1-%E7%94%BB%E5%9B%BE%E9%A2%98%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/">系统分析与设计 - 画图题注意事项</a>
          </li>
        
          <li>
            <a href="/2019/07/01/Attentive-Feedback-Network-for-Boundary-Aware-Salient-Object-Detection%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">Attentive Feedback Network for Boundary-Aware Salient Object Detection论文阅读笔记</a>
          </li>
        
          <li>
            <a href="/2019/07/01/neural-style%E6%BA%90%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB/">neural style源代码阅读</a>
          </li>
        
          <li>
            <a href="/2019/06/29/%E7%B3%BB%E7%BB%9F%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%BE%E8%AE%A1-%E5%BB%BA%E6%A8%A1%E9%83%A8%E5%88%86/">系统分析与设计 - 建模部分</a>
          </li>
        
          <li>
            <a href="/2019/06/28/Gatys-Image-Style-Transfer-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">Gatys - Image Style Transfer 论文阅读笔记</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>